################################################################################
# GSD-Gemini Multi-Backend Configuration
# ============================================================================
# Copy this file to .env and fill in your actual credentials
# DO NOT commit .env to git (already in .gitignore)
################################################################################

################################################################################
# 1. AGENT BACKEND SELECTOR
################################################################################

# Which backend to use: groq | gemini | github
# - groq: Groq Cloud (free tier: 12k TPM, 100k TPD)
# - gemini: Gemini CLI (local, requires Google Cloud SDK)
# - github: GitHub Models (Azure-hosted via GitHub, beta)
AGENT_BACKEND=groq

# Enable debug mode for verbose logging to .gsdgc/debug.log
DEBUG_MODE=false

# Enable auto-mode for continuous agent loop (keeps running until complete)
AUTO_MODE=false

################################################################################
# 2. GROQ BACKEND CONFIGURATION (RECOMMENDED FOR STARTING)
################################################################################
# Free tier: 12,000 tokens/minute, 100,000 tokens/day
# Get key at: https://console.groq.com/keys
# Rate limits reset daily at midnight UTC

# Groq API key (required for this backend)
GROQ_API_KEY=gsk_YOUR_API_KEY_HERE

# API endpoint (standard, usually don't change)
GROQ_API_URL=https://api.groq.com/openai/v1/chat/completions

# Model selection - Available models:
#   - llama-3.3-70b-versatile (RECOMMENDED: best quality, ~1 token/word)
#   - llama-3.2-90b-text-preview (multi-lingual support, ~1.2 token/word)
#   - gemma2-9b-it (fast, lightweight, ~0.8 token/word, less capable)
#   - mixtral-8x7b-32768 (balanced, good for code, ~0.9 token/word)
GROQ_MODEL=llama-3.3-70b-versatile

# Model behavior parameters
GROQ_TEMPERATURE=0.4          # 0.0=deterministic, 2.0=creative (use 0.4 for tasks)
GROQ_MAX_TOKENS=2000          # Max tokens per response (balance cost vs capability)

# Retry and backoff configuration
GROQ_RETRY_LIMIT=3            # Number of retry attempts on rate limit/error
GROQ_BACKOFF_INITIAL=2        # Initial backoff in seconds (doubles each retry)
GROQ_BACKOFF_MAX=30           # Maximum backoff seconds (safety limit)

# Operational delays (helps avoid cascading rate limits)
GROQ_BETWEEN_TASKS_SECONDS=2  # Delay between task executions in orchestrator
GROQ_BETWEEN_ITERATIONS_SECONDS=2  # Delay between agent iterations
GROQ_BETWEEN_ACTIONS_SECONDS=1  # Delay between action executions (bash, write, read)

# Cost estimates:
#   - Free tier: ~300-400 simple tasks/day or ~50-100 complex tasks/day
#   - Typical task: 150-300 tokens with optimized prompts
#   - Auto-mode continuous loop: ~20-30 full tasks/day

################################################################################
# 3. GEMINI CLI BACKEND CONFIGURATION (LOCAL EXECUTION)
################################################################################
# Requires: Google Cloud SDK with gcloud CLI installed
# Setup: https://cloud.google.com/sdk/docs/install
# Then: gcloud auth application-default login

# Path to gemini CLI executable (or just "gemini" if in PATH)
# Installation:
#   1. Install Google Cloud SDK
#   2. Run: gcloud auth application-default login
#   3. Verify: which gemini || echo "Not in PATH"
GEMINI_CLI_PATH=gemini

# Model selection - Available models:
#   - gemini-2.0-flash (RECOMMENDED: fast, latest)
#   - gemini-1.5-pro (high quality, supports documents/images)
#   - gemini-1.5-flash (lightweight, good balance)
GEMINI_MODEL=gemini-2.0-flash

# Model behavior parameters
GEMINI_TEMPERATURE=0.4        # 0.0=deterministic, 2.0=creative
GEMINI_MAX_TOKENS=2000        # Max tokens per response

# Operational delays (Gemini usually faster than Groq)
GEMINI_BETWEEN_TASKS_SECONDS=1  # Can be shorter; no API rate limits
GEMINI_BETWEEN_ITERATIONS_SECONDS=1
GEMINI_BETWEEN_ACTIONS_SECONDS=0.5

# Advantages:
#   - No API rate limits (depends on Google Cloud quota)
#   - Local execution (data privacy)
#   - Free tier available for development
#   - Multimodal support (images, documents)
#
# Cost: Typically $0.01-0.05/task (varies by Google Cloud project tier)

################################################################################
# 4. GITHUB MODELS BACKEND CONFIGURATION (BETA)
################################################################################
# Azure-hosted inference via GitHub Copilot integration
# Marketplace: https://github.com/marketplace/models
# Beta status: May have availability issues or rate limits

# GitHub Personal Access Token (required)
# Create at: https://github.com/settings/tokens/new
#   Scopes needed: "models" (or minimum "read:user")
# Fine-grained alternative: https://github.com/settings/personal-access-tokens/new
#   Permissions: Repository access "All repositories" + "Models" (read)
GITHUB_TOKEN=ghu_YOUR_TOKEN_HERE

# GitHub Models API endpoint (Azure-hosted, no changes usually needed)
GITHUB_MODELS_API_URL=https://models.inference.ai.azure.com/chat/completions

# Model selection - Check marketplace for available models:
#   - gpt-4o (RECOMMENDED: best quality, ~$0.03/task)
#   - gpt-4-turbo (high quality, faster)
#   - claude-3-5-sonnet (via GitHub partnership, ~$0.02/task)
#   - llama-3.1-70b (via GitHub partnership, open source)
#   - mistral-large (via partnership)
GITHUB_MODEL=gpt-4o

# Model behavior parameters
GITHUB_TEMPERATURE=0.4        # 0.0=deterministic, 2.0=creative
GITHUB_MAX_TOKENS=2000        # Max tokens per response

# Operational delays
GITHUB_BETWEEN_TASKS_SECONDS=1
GITHUB_BETWEEN_ITERATIONS_SECONDS=1
GITHUB_BETWEEN_ACTIONS_SECONDS=0.5

# Advantages:
#   - Access to latest models (GPT-4o, Claude-3.5, etc.)
#   - Fast inference (Azure-hosted)
#   - Pay-as-you-go pricing
#
# Disadvantages:
#   - Beta service (may have issues)
#   - Requires GitHub account + token
#   - Pricing varies by model ($0.01-0.05/task typical)
#
# Cost: Varies significantly by model chosen

################################################################################
# 5. USAGE GUIDE
################################################################################

# Quick setup:
#   1. Copy this file: cp .env.example .env
#   2. Edit .env with your credentials (never commit it!)
#   3. Test with: ./gsdgc "create a hello.py file"

# Switching backends:
#   export AGENT_BACKEND=groq   # or gemini, or github
#   ./gsdgc "your task"

# Monitoring:
#   - Backend dashboards:
#     * Groq: https://console.groq.com/usage
#     * Gemini: Google Cloud Console > Generative AI API
#     * GitHub: https://github.com/settings/billing/overview
#   - System logs: cat .gsdgc/debug.log (if DEBUG_MODE=true)

# Troubleshooting:
#   - "Rate limited": Increase delay variables, wait for quota reset
#   - "Invalid API key": Verify key from backend dashboard
#   - "Command not found": Check GEMINI_CLI_PATH or install dependencies
#   - "Backend error": Enable DEBUG_MODE=true to see detailed logs

################################################################################
# 6. COST COMPARISON
################################################################################

# Approximate costs per task (simple project creation):
#
# Groq (free tier recommended):
#   - Free: 12k TPM, 100k TPD (unlimited tasks within quota)
#   - Typical task: 200-300 tokens â†’ 1000+ free tasks/day
#
# Gemini CLI (Google Cloud):
#   - Free tier: 60 requests/minute, modest quota
#   - Paid: ~$0.01-0.05/task depending on tier
#
# GitHub Models (Azure-hosted):
#   - Pay-as-you-go: ~$0.01-0.05/task (varies by model)
#   - Recommended for testing specific models (GPT-4o, Claude)

################################################################################
# 7. BEST PRACTICES
################################################################################

# - Start with Groq (free tier is generous for experimentation)
# - Use DEBUG_MODE=true during setup to troubleshoot issues
# - Set shorter delays once stable (GROQ_BETWEEN_* variables)
# - Keep .env in .gitignore (it contains credentials!)
# - Monitor backend usage to avoid unexpected costs
# - Document decisions in .gsdgc/decisions.md for reference

################################################################################
